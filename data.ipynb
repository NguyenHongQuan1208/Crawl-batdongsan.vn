{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import urllib.parse as urlparse\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://batdongsan.vn/\"\n",
    "t = \"/ban-nha/p\"\n",
    "r = requests.get(link+t[0])\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link(soup, link):\n",
    "    l = []\n",
    "    div_items = soup.find_all(\"div\", attrs={\"class\" : \"item\"})\n",
    "    for div_item in div_items:\n",
    "        a_links = div_item.find_all('a',href=True)\n",
    "        for a_link in a_links:\n",
    "            l.append(a_link['href'])\n",
    "    return l\n",
    "\n",
    "# for value in find_link(soup, link):\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data(soup):\n",
    "    data = {}\n",
    "\n",
    "    # Tiêu đề\n",
    "    titles = soup.find_all('div', {\"class\": \"name\"})\n",
    "    data['title'] = [title.find('a').text.strip() for title in titles]\n",
    "\n",
    "    # Giá\n",
    "    prices = soup.find_all('span', {\"class\": \"price\"})\n",
    "    data['price'] = [price.text.strip() for price in prices]\n",
    "\n",
    "    # Diện tích\n",
    "    acreages = soup.find_all('span', {\"class\": \"acreage\"})\n",
    "    data['acreage'] = [acreage.text.strip() for acreage in acreages]\n",
    "\n",
    "    # Mô tả\n",
    "    descriptions = soup.find_all('div', {\"class\": \"sapo\"})\n",
    "    data['description'] = [description.find('p').text.strip() for description in descriptions]\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_property_elements(soup):\n",
    "    data_list = []\n",
    "\n",
    "    # Tìm tất cả các phần tử HTML chứa thông tin\n",
    "    property_elements = soup.find_all('div', {\"class\": \"item\"})\n",
    "\n",
    "    for property_element in property_elements:\n",
    "        # Gọi hàm find_data cho mỗi phần tử và thêm kết quả vào danh sách\n",
    "        data_list.append(find_data(property_element))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def scrape_and_save(link, base_link, t, max_pages):\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(1, max_pages + 1):\n",
    "        try:\n",
    "            r = requests.get(link + t + str(i))\n",
    "            r.raise_for_status()  # Raise HTTPError for bad requests\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "            property_data = process_property_elements(soup)\n",
    "            all_data.extend(property_data)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {i}: {e}\")\n",
    "\n",
    "    with open('bat_dong_san.json', 'w') as file:\n",
    "        json.dump(all_data, file)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "max_pages = 50\n",
    "\n",
    "scrape_and_save(link, link, t, max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data from page 1\n",
      "No data to save.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://batdongsan.com.vn/nha-dat-ban\"\n",
    "\n",
    "\n",
    "def extract_data_from_product(product):\n",
    "    def extract_element_text(parent, selector):\n",
    "        element = parent.select_one(selector)\n",
    "        return element.text.strip() if element else None\n",
    "\n",
    "    def extract_image_src(parent, selector):\n",
    "        element = parent.select_one(selector)\n",
    "        return element[\"src\"] if element else None\n",
    "\n",
    "    data = {\n",
    "        \"prid\": product.get(\"prid\"),\n",
    "        \"title\": product.select_one(\"a.js__product-link-for-product-id\").get(\"title\") if product.select_one(\"a.js__product-link-for-product-id\") else None,\n",
    "        \"location\": extract_element_text(product, \"div.re__card-location\"),\n",
    "        \"time_published\": product.select_one(\"span.re__card-published-info-published-at\").get(\"aria-label\") if product.select_one(\"span.re__card-published-info-published-at\") else None,\n",
    "        \"price\": extract_element_text(product, \"span.re__card-config-price\"),\n",
    "        \"area\": extract_element_text(product, \"span.re__card-config-area\"),\n",
    "        \"price_per_m2\": extract_element_text(product, \"span.re__card-config-price_per_m2\"),\n",
    "        \"bedrooms\": extract_element_text(product, \"span.re__card-config-bedroom\"),\n",
    "        \"bathrooms\": extract_element_text(product, \"span.re__card-config-toilet\"),\n",
    "        \"agent_name\": extract_element_text(product, \"div.re__card-published-info-agent-profile-name\"),\n",
    "        \"product_url\": base_url + product.select_one(\"a.js__product-link-for-product-id\").get(\"href\") if product.select_one(\"a.js__product-link-for-product-id\") else None,\n",
    "        \"agent_avatar\": extract_image_src(product, \"div.re__card-published-info-agent-profile-avatar img\") if extract_image_src(product, \"div.re__card-published-info-agent-profile-avatar img\") else None,\n",
    "        \"main_image\": product.select_one(\"div.re__img-child img[data-img]\")[\"data-img\"] if product.select_one(\"div.re__img-child img[data-img]\") else None,\n",
    "        \"other_images\": [img[\"data-src\"] for img in product.select(\"div.re__img-child img[data-src]\")],\n",
    "        \"description\": extract_element_text(product, \"div.re__card-description.js__card-description\"),\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def crawl_with_zenrows(url):\n",
    "    all_products = []\n",
    "    with tqdm(total=50) as pbar:\n",
    "        for page_number in range(1, 51):\n",
    "            params = {\n",
    "                'url': f\"{url}/p{page_number}\",\n",
    "                'apikey': 'fbda2bc1d8a35edf5bfdf3f2dad6b66de5c73e50',\n",
    "                'premium_proxy': 'true',\n",
    "            }\n",
    "            response = requests.get(\n",
    "                'https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                html_content = response.text\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                product_elements = soup.find_all(\"div\", {\"class\": \"js__card\"})\n",
    "\n",
    "                for product in product_elements:\n",
    "                    all_products.append(extract_data_from_product(product))\n",
    "            else:\n",
    "                print(f\"Failed to fetch data from page {page_number}\")\n",
    "                break\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    return all_products\n",
    "\n",
    "\n",
    "all_products = crawl_with_zenrows(base_url)\n",
    "\n",
    "if all_products:\n",
    "    df = pd.DataFrame(all_products)\n",
    "\n",
    "    df.to_csv(\"products_data.csv\", index=False)\n",
    "    print(\"Data from 50 pages saved to 'products_data.csv'\")\n",
    "    df.to_json(\"products_data.json\", orient=\"records\")\n",
    "    print(\"Data from 50 pages saved to 'products_data.json'\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
