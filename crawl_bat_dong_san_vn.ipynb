{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import urllib.parse as urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://batdongsan.vn/\"\n",
    "t = \"/ban-nha/p\"\n",
    "r = requests.get(link+t[0])\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link(soup):\n",
    "    l = []\n",
    "    div_items = soup.find_all(\"div\", attrs={\"class\" : \"item\"})\n",
    "    for div_item in div_items:\n",
    "        a_links = div_item.find_all('a',href=True)\n",
    "        for a_link in a_links:\n",
    "            l.append(a_link['href'])\n",
    "    return l\n",
    "\n",
    "# for value in find_link(soup):\n",
    "#     print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data(soup):\n",
    "    data = {}\n",
    "\n",
    "    # Tiêu đề\n",
    "    titles = soup.find_all('div', {\"class\": \"name\"})\n",
    "    data['title'] = titles[0].find('a').text.strip() if titles else None\n",
    "\n",
    "    # Giá\n",
    "    prices = soup.find_all('span', {\"class\": \"price\"})\n",
    "    data['price'] = prices[0].text.strip() if prices else None\n",
    "\n",
    "    # Diện tích\n",
    "    acreages = soup.find_all('span', {\"class\": \"acreage\"})\n",
    "    data['acreage'] = acreages[0].text.strip() if acreages else None\n",
    "\n",
    "    # Mô tả\n",
    "    descriptions = soup.find_all('div', {\"class\": \"sapo\"})\n",
    "    data['description'] = descriptions[0].find('p').text.strip() if descriptions else None\n",
    "\n",
    "    # Ảnh\n",
    "    images = soup.find_all('div', {\"class\": \"image cover\"})\n",
    "    image_tag = images[0].find('img') if images else None\n",
    "    data['image_url'] = image_tag['src'] if image_tag and 'src' in image_tag.attrs else None\n",
    "\n",
    "    # Link sản phẩm\n",
    "    link_tags = images[0].find_all('a') if images else None\n",
    "    data['product_link'] = link_tags[0]['href'] if link_tags else None\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_property_elements(soup):\n",
    "    data_list = []\n",
    "\n",
    "    # Tìm tất cả các phần tử HTML chứa thông tin\n",
    "    property_elements = soup.find_all('div', {\"class\": \"item\"})\n",
    "\n",
    "    for property_element in property_elements:\n",
    "        # Gọi hàm find_data cho mỗi phần tử và thêm kết quả vào danh sách\n",
    "        data_list.append(find_data(property_element))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def scrape_and_save(link, base_link, t, max_pages):\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(1, max_pages + 1):\n",
    "        try:\n",
    "            r = requests.get(link + t + str(i))\n",
    "            r.raise_for_status()  # Raise HTTPError for bad requests\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "            property_data = process_property_elements(soup)\n",
    "            all_data.extend(property_data)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {i}: {e}\")\n",
    "\n",
    "    with open('bat_dong_san_vn.json', 'w') as file:\n",
    "        json.dump(all_data, file)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "max_pages = 50\n",
    "\n",
    "scrape_and_save(link, link, t, max_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
